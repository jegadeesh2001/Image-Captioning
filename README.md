<div align="center">
<h1> Image Captioning
</h1>

<p>
Given a image from the user or live capture, we will pass the image to the trained image captioning model. The returned captions which contains the context of image is converted  to speech and displayed to user using streamlit
</p>

<p align="center">
   <img src="https://skillicons.dev/icons?i=python,github" />
</p>

![Streamlit](https://img.shields.io/static/v1?style=for-the-badge&message=Streamlit&color=FF4B4B&logo=Streamlit&logoColor=FFFFFF&label=)
<hr>
</div>

![screencapture-localhost-8501-Image-Captioning-2023-03-24-13_14_07](https://user-images.githubusercontent.com/62760269/227456754-d682aa0a-c918-4d77-afb8-0ee78c4a458a.png)


 ## Tasks Done
- [x] Extracting Images from https://github.com/goodwillyoga/Flickr8k_dataset
- [x] Image Preprocessing
- [x] Text Preprocessing
- [x] Generating Vocabulary of Words for Prediction
- [x] Exploratory Data Analysis
- [x] Extracting Features from Images using Xception
- [x] Merge Architecture using LSTM and CNN for Image Captioning
- [x] Model Training
- [x] Model Evaluation on Test data using BLEU Score
- [x] Displaying the end results in Web App built using Streamlit


## Datasets and Pre-trained Models Used
* https://github.com/goodwillyoga/Flickr8k_dataset
* https://keras.io/api/applications/xception/

## Complete Presentation on Image Captioning
* https://docs.google.com/presentation/d/1G71wjDyuwb3JDAHYcx9bw7T700WjuVD6kIvX39Lq_Eo/edit?usp=sharing
